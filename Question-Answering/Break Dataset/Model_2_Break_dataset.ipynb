{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Model-2_Break_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karanamrahul/END-Course-NLP/blob/main/Model_2_Break_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG9Znkhh6ydz"
      },
      "source": [
        "# 3 - Neural Machine Translation by Jointly Learning to Align and Translate\r\n",
        "\r\n",
        "In this third notebook on sequence-to-sequence models using PyTorch and TorchText, we'll be implementing the model from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473). This model achives our best perplexity yet, ~27 compared to ~34 for the previous model.\r\n",
        "\r\n",
        "## Introduction\r\n",
        "\r\n",
        "As a reminder, here is the general encoder-decoder model:\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq1.png?raw=1)\r\n",
        "\r\n",
        "In the previous model, our architecture was set-up in a way to reduce \"information compression\" by explicitly passing the context vector, $z$, to the decoder at every time-step and by passing both the context vector and embedded input word, $d(y_t)$, along with the hidden state, $s_t$, to the linear layer, $f$, to make a prediction.\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png?raw=1)\r\n",
        "\r\n",
        "Even though we have reduced some of this compression, our context vector still needs to contain all of the information about the source sentence. The model implemented in this notebook avoids this compression by allowing the decoder to look at the entire source sentence (via its hidden states) at each decoding step! How does it do this? It uses *attention*. \r\n",
        "\r\n",
        "Attention works by first, calculating an attention vector, $a$, that is the length of the source sentence. The attention vector has the property that each element is between 0 and 1, and the entire vector sums to 1. We then calculate a weighted sum of our source sentence hidden states, $H$, to get a weighted source vector, $w$. \r\n",
        "\r\n",
        "$$w = \\sum_{i}a_ih_i$$\r\n",
        "\r\n",
        "We calculate a new weighted source vector every time-step when decoding, using it as input to our decoder RNN as well as the linear layer to make a prediction. We'll explain how to do all of this during the session.\r\n",
        "\r\n",
        "## Preparing Data\r\n",
        "\r\n",
        "Again, the preparation is similar to last time.\r\n",
        "\r\n",
        "First we import all the required modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lsU0zHgJ4FM"
      },
      "source": [
        "DATA_DIR = '/content/Break_dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwmCBqMZKdar"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "validation_data = pd.read_csv(os.path.join(DATA_DIR, 'dev.csv'), sep=',')\r\n",
        "train_data = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), sep=',')\r\n",
        "#test_data = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YChkrP54K--S"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "from torchtext import data \r\n",
        "\r\n",
        "import spacy\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUl9Ok5pLWbe"
      },
      "source": [
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j3rNHeM5fm9"
      },
      "source": [
        "question = Field(tokenize='spacy',init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True)\r\n",
        "\r\n",
        "decomposition = Field(tokenize='spacy',init_token='<sos>',\r\n",
        "                      eos_token = '<eos>',\r\n",
        "                      lower=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfw5E2xO6e4D"
      },
      "source": [
        "fields = [('questions', question),('decompositions',decomposition)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgzcDqU66rnn"
      },
      "source": [
        "example_train = [data.Example.fromlist([train_data.question_text[i],train_data.decomposition[i]], fields) for i in range(train_data.shape[0])] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p64xSpOEhSuT"
      },
      "source": [
        "example_val = [data.Example.fromlist([validation_data.question_text[i],validation_data.decomposition[i]],fields) for i in range(validation_data.shape[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1N6B5Y6m4TD"
      },
      "source": [
        "train = data.Dataset(example_train,fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFzlC06Uple8"
      },
      "source": [
        "val = data.Dataset(example_val,fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeyRH0h3rQuC",
        "outputId": "6df78ef6-b5ce-432c-e790-e3e5730d5f35"
      },
      "source": [
        "print(f\"Number of training examples: {len(train.examples)}\")\r\n",
        "print(f\"Number of validation examples: {len(val.examples)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 44321\n",
            "Number of validation examples: 7760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69EKY-5StUWO",
        "outputId": "e24a5770-b449-49ff-b1c5-9e78703b0887"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM5eDGnep54P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02b7048-71c1-473d-a543-6e39b8e2e39d"
      },
      "source": [
        "question.build_vocab(train,min_freq=2, vectors='glove.6B.200d')\r\n",
        "\r\n",
        "decomposition.build_vocab(train,min_freq=2,vectors='glove.6B.200d')\r\n",
        "\r\n",
        "\r\n",
        "#TEXT.build_vocab(trn, vectors=GloVe(name='6B', dim=300))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                          \n",
            "100%|█████████▉| 399991/400000 [00:26<00:00, 15172.67it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb143b2fqUcL",
        "outputId": "c9159423-ff01-4234-c35c-2cdcd945cf72"
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(question.vocab)}\")\r\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(decomposition.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 9494\n",
            "Unique tokens in target (en) vocabulary: 9499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNXR0hPtsaI9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2689d078-ac9e-481b-b830-9219bd0ff15d"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCqf10Ewss0l"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\r\n",
        "    (train, val), \r\n",
        "    batch_size = BATCH_SIZE, \r\n",
        "    device = device,sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_nH2imAs1ve"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src):\r\n",
        "        \r\n",
        "        #src = [src len, batch size]\r\n",
        "        \r\n",
        "        embedded = self.dropout(self.embedding(src))\r\n",
        "        \r\n",
        "        #embedded = [src len, batch size, emb dim]\r\n",
        "        \r\n",
        "        outputs, hidden = self.rnn(embedded)\r\n",
        "                \r\n",
        "        #outputs = [src len, batch size, hid dim * num directions]\r\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\r\n",
        "        \r\n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\r\n",
        "        #outputs are always from the last layer\r\n",
        "        \r\n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \r\n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\r\n",
        "        \r\n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \r\n",
        "        #  encoder RNNs fed through a linear layer\r\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\r\n",
        "        \r\n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\r\n",
        "        #hidden = [batch size, dec hid dim]\r\n",
        "        \r\n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owpFS6bu66Rr"
      },
      "source": [
        "## Building the Seq2Seq Model\r\n",
        "\r\n",
        "### Encoder\r\n",
        "\r\n",
        "First, we'll build the encoder. Similar to the previous model, we only use a single layer GRU, however we now use a *bidirectional RNN*. With a bidirectional RNN, we have two RNNs in each layer. A *forward RNN* going over the embedded sentence from left to right (shown below in green), and a *backward RNN* going over the embedded sentence from right to left (teal). All we need to do in code is set `bidirectional = True` and then pass the embedded sentence to the RNN as before. \r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq8.png?raw=1)\r\n",
        "\r\n",
        "We now have:\r\n",
        "\r\n",
        "$$\\begin{align*}\r\n",
        "h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\r\n",
        "h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\r\n",
        "\\end{align*}$$\r\n",
        "\r\n",
        "Where $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ and $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\r\n",
        "\r\n",
        "As before, we only pass an input (`embedded`) to the RNN, which tells PyTorch to initialize both the forward and backward initial hidden states ($h_0^\\rightarrow$ and $h_0^\\leftarrow$, respectively) to a tensor of all zeros. We'll also get two context vectors, one from the forward RNN after it has seen the final word in the sentence, $z^\\rightarrow=h_T^\\rightarrow$, and one from the backward RNN after it has seen the first word in the sentence, $z^\\leftarrow=h_T^\\leftarrow$.\r\n",
        "\r\n",
        "The RNN returns `outputs` and `hidden`. \r\n",
        "\r\n",
        "`outputs` is of size **[src len, batch size, hid dim * num directions]** where the first `hid_dim` elements in the third axis are the hidden states from the top layer forward RNN, and the last `hid_dim` elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other, i.e. $h_1 = [h_1^\\rightarrow; h_{T}^\\leftarrow]$, $h_2 = [h_2^\\rightarrow; h_{T-1}^\\leftarrow]$ and we can denote all encoder hidden states (forward and backwards concatenated together) as $H=\\{ h_1, h_2, ..., h_T\\}$.\r\n",
        "\r\n",
        "`hidden` is of size **[n layers * num directions, batch size, hid dim]**, where **[-2, :, :]** gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and **[-1, :, :]** gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\r\n",
        "\r\n",
        "As the decoder is not bidirectional, it only needs a single context vector, $z$, to use as its initial hidden state, $s_0$, and we currently have two, a forward and a backward one ($z^\\rightarrow=h_T^\\rightarrow$ and $z^\\leftarrow=h_T^\\leftarrow$, respectively). We solve this by concatenating the two context vectors together, passing them through a linear layer, $g$, and applying the $\\tanh$ activation function. \r\n",
        "\r\n",
        "$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\r\n",
        "\r\n",
        "**Note**: this is actually a deviation from the paper. Instead, they feed only the first backward RNN hidden state through a linear layer to get the context vector/decoder initial hidden state. This doesn't seem to make sense to me, so we have changed it.\r\n",
        "\r\n",
        "As we want our model to look back over the whole of the source sentence we return `outputs`, the stacked forward and backward hidden states for every token in the source sentence. We also return `hidden`, which acts as our initial hidden state in the decoder.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RpQpwHQs6DZ"
      },
      "source": [
        "class Attention(nn.Module):\r\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\r\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\r\n",
        "        \r\n",
        "    def forward(self, hidden, encoder_outputs):\r\n",
        "        \r\n",
        "        #hidden = [batch size, dec hid dim]\r\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n",
        "        \r\n",
        "        batch_size = encoder_outputs.shape[1]\r\n",
        "        src_len = encoder_outputs.shape[0]\r\n",
        "        \r\n",
        "        #repeat decoder hidden state src_len times\r\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\r\n",
        "        \r\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n",
        "        \r\n",
        "        #hidden = [batch size, src len, dec hid dim]\r\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n",
        "        \r\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \r\n",
        "        \r\n",
        "        #energy = [batch size, src len, dec hid dim]\r\n",
        "\r\n",
        "        attention = self.v(energy).squeeze(2)\r\n",
        "        \r\n",
        "        #attention= [batch size, src len]\r\n",
        "        \r\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eNy035MaSEI"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.attention = attention\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, input, hidden, encoder_outputs):\r\n",
        "             \r\n",
        "        #input = [batch size]\r\n",
        "        #hidden = [batch size, dec hid dim]\r\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n",
        "        \r\n",
        "        input = input.unsqueeze(0)\r\n",
        "        \r\n",
        "        #input = [1, batch size]\r\n",
        "        \r\n",
        "        embedded = self.dropout(self.embedding(input))\r\n",
        "        \r\n",
        "        #embedded = [1, batch size, emb dim]\r\n",
        "        \r\n",
        "        a = self.attention(hidden, encoder_outputs)\r\n",
        "                \r\n",
        "        #a = [batch size, src len]\r\n",
        "        \r\n",
        "        a = a.unsqueeze(1)\r\n",
        "        \r\n",
        "        #a = [batch size, 1, src len]\r\n",
        "        \r\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n",
        "        \r\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n",
        "        \r\n",
        "        weighted = torch.bmm(a, encoder_outputs)\r\n",
        "        \r\n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\r\n",
        "        \r\n",
        "        weighted = weighted.permute(1, 0, 2)\r\n",
        "        \r\n",
        "        #weighted = [1, batch size, enc hid dim * 2]\r\n",
        "        \r\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\r\n",
        "        \r\n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\r\n",
        "            \r\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\r\n",
        "        \r\n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\r\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\r\n",
        "        \r\n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\r\n",
        "        #output = [1, batch size, dec hid dim]\r\n",
        "        #hidden = [1, batch size, dec hid dim]\r\n",
        "        #this also means that output == hidden\r\n",
        "        assert (output == hidden).all()\r\n",
        "        \r\n",
        "        embedded = embedded.squeeze(0)\r\n",
        "        output = output.squeeze(0)\r\n",
        "        weighted = weighted.squeeze(0)\r\n",
        "        \r\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\r\n",
        "        \r\n",
        "        #prediction = [batch size, output dim]\r\n",
        "        \r\n",
        "        return prediction, hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ITeJEMytBrR"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\r\n",
        "        \r\n",
        "        #src = [src len, batch size]\r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\r\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\r\n",
        "        \r\n",
        "        batch_size = src.shape[1]\r\n",
        "        trg_len = trg.shape[0]\r\n",
        "        trg_vocab_size = self.decoder.output_dim\r\n",
        "        \r\n",
        "        #tensor to store decoder outputs\r\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n",
        "        \r\n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\r\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\r\n",
        "        encoder_outputs, hidden = self.encoder(src)\r\n",
        "                \r\n",
        "        #first input to the decoder is the <sos> tokens\r\n",
        "        input = trg[0,:]\r\n",
        "        \r\n",
        "        for t in range(1, trg_len):\r\n",
        "            \r\n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\r\n",
        "            #receive output tensor (predictions) and new hidden state\r\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\r\n",
        "            \r\n",
        "            #place predictions in a tensor holding predictions for each token\r\n",
        "            outputs[t] = output\r\n",
        "            \r\n",
        "            #decide if we are going to use teacher forcing or not\r\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\r\n",
        "            \r\n",
        "            #get the highest predicted token from our predictions\r\n",
        "            top1 = output.argmax(1) \r\n",
        "            \r\n",
        "            #if teacher forcing, use actual next token as next input\r\n",
        "            #if not, use predicted token\r\n",
        "            input = trg[t] if teacher_force else top1\r\n",
        "\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFTBquZhtFdf"
      },
      "source": [
        "INPUT_DIM = len(question.vocab)\r\n",
        "OUTPUT_DIM = len(decomposition.vocab)\r\n",
        "ENC_EMB_DIM = 256\r\n",
        "DEC_EMB_DIM = 256\r\n",
        "ENC_HID_DIM = 512\r\n",
        "DEC_HID_DIM = 512\r\n",
        "ENC_DROPOUT = 0.5\r\n",
        "DEC_DROPOUT = 0.5\r\n",
        "\r\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\r\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\r\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM_LUKZ6tVSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a667c8-79ed-43b7-d40c-3653ddcbb930"
      },
      "source": [
        "def init_weights(m):\r\n",
        "    for name, param in m.named_parameters():\r\n",
        "        if 'weight' in name:\r\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\r\n",
        "        else:\r\n",
        "            nn.init.constant_(param.data, 0)\r\n",
        "            \r\n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(9494, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(9499, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=9499, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U81yPWg51TU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8170cf26-64c2-445c-c9e8-305175f47a12"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 28,327,195 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9hXoYsY5430"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rlnTDaT55wE"
      },
      "source": [
        "TRG_PAD_IDX = decomposition.vocab.stoi[decomposition.pad_token]\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEyFTjmc5_Pu"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.questions\r\n",
        "        trg = batch.decompositions\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output = model(src, trg)\r\n",
        "        \r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #output = [trg len, batch size, output dim]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output[1:].view(-1, output_dim)\r\n",
        "        trg = trg[1:].view(-1)\r\n",
        "        \r\n",
        "        #trg = [(trg len - 1) * batch size]\r\n",
        "        #output = [(trg len - 1) * batch size, output dim]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4l06fBo6HuA"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.questions\r\n",
        "            trg = batch.decompositions\r\n",
        "\r\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\r\n",
        "\r\n",
        "            #trg = [trg len, batch size]\r\n",
        "            #output = [trg len, batch size, output dim]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output[1:].view(-1, output_dim)\r\n",
        "            trg = trg[1:].view(-1)\r\n",
        "\r\n",
        "            #trg = [(trg len - 1) * batch size]\r\n",
        "            #output = [(trg len - 1) * batch size, output dim]\r\n",
        "\r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBrpvDKs6LpU"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK4Otdwr9tF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55261ba2-f3ed-4d61-c962-9180d1ec611a"
      },
      "source": [
        "print(vars(val.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'questions': ['what', 'flights', 'are', 'available', 'tomorrow', 'from', 'denver', 'to', 'philadelphia'], 'decompositions': ['return', 'flights', ';', 'return', '#', '1', 'from', ' ', 'denver', ';', 'return', '#', '2', 'to', 'philadelphia', ';', 'return', '#', '3', 'if', ' ', 'available']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "cYuU7KkRQ6cu",
        "outputId": "e148dfd1-c957-45fe-caaa-7017bb41d58b"
      },
      "source": [
        "torch.cuda.empty_cache()\r\n",
        "\r\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  110653 KB |  110653 KB |  130657 KB |   20004 KB |\\n|       from large pool |  110610 KB |  110610 KB |  130578 KB |   19968 KB |\\n|       from small pool |      43 KB |      43 KB |      79 KB |      36 KB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  110653 KB |  110653 KB |  130657 KB |   20004 KB |\\n|       from large pool |  110610 KB |  110610 KB |  130578 KB |   19968 KB |\\n|       from small pool |      43 KB |      43 KB |      79 KB |      36 KB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  131072 KB |  131072 KB |  131072 KB |       0 B  |\\n|       from large pool |  129024 KB |  129024 KB |  129024 KB |       0 B  |\\n|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   20418 KB |   24268 KB |   60203 KB |   39784 KB |\\n|       from large pool |   18414 KB |   22226 KB |   56085 KB |   37671 KB |\\n|       from small pool |    2004 KB |    2046 KB |    4118 KB |    2113 KB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      11    |      13    |      23    |      12    |\\n|       from large pool |       7    |       8    |      13    |       6    |\\n|       from small pool |       4    |       5    |      10    |       6    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      11    |      13    |      23    |      12    |\\n|       from large pool |       7    |       8    |      13    |       6    |\\n|       from small pool |       4    |       5    |      10    |       6    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       5    |       5    |       5    |       0    |\\n|       from large pool |       4    |       4    |       4    |       0    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       6    |       6    |      10    |       4    |\\n|       from large pool |       5    |       5    |       6    |       1    |\\n|       from small pool |       1    |       2    |       4    |       3    |\\n|===========================================================================|\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w-jh74n6OQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da90b6c2-1aac-4053-f68e-f2d0aba1757e"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399991/400000 [00:39<00:00, 15172.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 9m 20s\n",
            "\tTrain Loss: 2.678 | Train PPL:  14.558\n",
            "\t Val. Loss: 4.147 |  Val. PPL:  63.232\n",
            "Epoch: 02 | Time: 9m 13s\n",
            "\tTrain Loss: 1.817 | Train PPL:   6.156\n",
            "\t Val. Loss: 4.152 |  Val. PPL:  63.542\n",
            "Epoch: 03 | Time: 9m 22s\n",
            "\tTrain Loss: 1.452 | Train PPL:   4.273\n",
            "\t Val. Loss: 3.981 |  Val. PPL:  53.553\n",
            "Epoch: 04 | Time: 9m 20s\n",
            "\tTrain Loss: 1.219 | Train PPL:   3.385\n",
            "\t Val. Loss: 3.984 |  Val. PPL:  53.717\n",
            "Epoch: 05 | Time: 9m 18s\n",
            "\tTrain Loss: 1.063 | Train PPL:   2.894\n",
            "\t Val. Loss: 3.988 |  Val. PPL:  53.961\n",
            "Epoch: 06 | Time: 9m 19s\n",
            "\tTrain Loss: 0.963 | Train PPL:   2.619\n",
            "\t Val. Loss: 4.036 |  Val. PPL:  56.582\n",
            "Epoch: 07 | Time: 9m 20s\n",
            "\tTrain Loss: 0.883 | Train PPL:   2.419\n",
            "\t Val. Loss: 4.171 |  Val. PPL:  64.778\n",
            "Epoch: 08 | Time: 9m 19s\n",
            "\tTrain Loss: 0.836 | Train PPL:   2.307\n",
            "\t Val. Loss: 4.121 |  Val. PPL:  61.645\n",
            "Epoch: 09 | Time: 9m 22s\n",
            "\tTrain Loss: 0.801 | Train PPL:   2.228\n",
            "\t Val. Loss: 4.278 |  Val. PPL:  72.065\n",
            "Epoch: 10 | Time: 9m 20s\n",
            "\tTrain Loss: 0.754 | Train PPL:   2.125\n",
            "\t Val. Loss: 4.253 |  Val. PPL:  70.283\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}